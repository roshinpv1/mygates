#!/usr/bin/env python3
"""
Setup script for local LLM configuration.
This will help you create the necessary .env file and test your setup.
"""

import os
from pathlib import Path

def create_env_file():
    """Create .env file with local LLM configuration"""
    
    env_content = """# CodeGates Local LLM Configuration
# This file was generated by setup_local_llm.py

# ==================================================
# Local LLM Configuration (LM Studio Default)
# ==================================================

# Local LLM service URL
LOCAL_LLM_URL=http://localhost:1234/v1

# Model name (update this to match your loaded model)
LOCAL_LLM_MODEL=meta-llama-3.1-8b-instruct

# API key (usually not needed for local services)
LOCAL_LLM_API_KEY=not-needed

# LLM settings
LOCAL_LLM_TEMPERATURE=0.1
LOCAL_LLM_MAX_TOKENS=4000

# ==================================================
# Alternative: Ollama Configuration
# ==================================================
# Uncomment these lines if using Ollama instead:
# OLLAMA_HOST=http://localhost:11434
# OLLAMA_MODEL=meta-llama-3.1-8b-instruct

# ==================================================
# Cloud LLM Providers (Optional)
# ==================================================
# Uncomment and configure if you prefer cloud-based LLM:
# OPENAI_API_KEY=sk-your-openai-api-key-here
# ANTHROPIC_API_KEY=sk-ant-your-anthropic-api-key-here

# ==================================================
# GitHub Integration (Optional)
# ==================================================
# GITHUB_TOKEN=ghp_your-github-token-here
"""

    env_file = Path(".env")
    
    if env_file.exists():
        print(f"‚ö†Ô∏è .env file already exists at {env_file.absolute()}")
        response = input("Do you want to overwrite it? (y/N): ")
        if response.lower() != 'y':
            print("Keeping existing .env file")
            return False
    
    with open(env_file, 'w') as f:
        f.write(env_content)
    
    print(f"‚úÖ Created .env file at {env_file.absolute()}")
    print(f"üìù Please edit this file to match your local LLM configuration")
    return True

def check_local_llm_services():
    """Check if any local LLM services are running"""
    import requests
    
    services = [
        {"name": "LM Studio", "url": "http://localhost:1234", "check_url": "http://localhost:1234/v1/models"},
        {"name": "Ollama", "url": "http://localhost:11434", "check_url": "http://localhost:11434/api/tags"},
    ]
    
    print("\nüîç Checking for running local LLM services...")
    
    found_any = False
    for service in services:
        try:
            response = requests.get(service["check_url"], timeout=5)
            if response.status_code == 200:
                print(f"   ‚úÖ {service['name']} is running at {service['url']}")
                found_any = True
            else:
                print(f"   ‚ö™ {service['name']} responded with status {response.status_code}")
        except requests.RequestException:
            print(f"   ‚ùå {service['name']} is not running at {service['url']}")
    
    if not found_any:
        print(f"\nüí° No local LLM services detected. You need to:")
        print(f"   1. Install and start LM Studio (https://lmstudio.ai/)")
        print(f"   2. Load a model like 'meta-llama-3.1-8b-instruct'")
        print(f"   3. Start the local server (usually on port 1234)")
        print(f"   4. Or install Ollama: https://ollama.com/")
    
    return found_any

def main():
    """Main setup function"""
    print("üöÄ CodeGates Local LLM Setup")
    print("=" * 40)
    
    # Step 1: Create .env file
    print("\nüìù Step 1: Creating environment configuration...")
    create_env_file()
    
    # Step 2: Check for local services
    print("\nüîç Step 2: Checking for local LLM services...")
    services_found = check_local_llm_services()
    
    # Step 3: Instructions
    print(f"\nüìã Step 3: Next Steps")
    print("=" * 40)
    
    if services_found:
        print("‚úÖ Local LLM service detected!")
        print("üéØ Now run the test script:")
        print("   python test_local_llm.py")
        print("üöÄ Or start using CodeGates with LLM:")
        print("   python start_server.py")
    else:
        print("‚ö†Ô∏è No local LLM service found. To set up:")
        print("")
        print("üöÄ Option 1 - LM Studio (Easiest):")
        print("   1. Download from https://lmstudio.ai/")
        print("   2. Install and open LM Studio")
        print("   3. Go to 'Discover' and download: meta-llama-3.1-8b-instruct")
        print("   4. Go to 'Local Server' and start the server")
        print("   5. Verify the port is 1234 (default)")
        print("")
        print("ü¶ô Option 2 - Ollama:")
        print("   1. Install: curl -fsSL https://ollama.com/install.sh | sh")
        print("   2. Pull model: ollama pull meta-llama-3.1-8b-instruct")
        print("   3. Update .env to use Ollama settings")
        print("")
        print("‚úÖ After setup, run: python test_local_llm.py")

if __name__ == "__main__":
    main() 